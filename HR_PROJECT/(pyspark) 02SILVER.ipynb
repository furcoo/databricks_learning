{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f53c77e-2bd0-4355-aca8-6a9653b53e0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#项目：HR_PROJECT\n",
    "##这是用来处理明细层的数据，用来清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d086b51-626b-4e49-95fa-1a0529ee9639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import col, current_timestamp,to_timestamp\n",
    "result_df = spark.table(\"uc_data001.uc_schema001.raw_hr_employees\")  # 员工表\n",
    "fct_hr_employees_df = DeltaTable.forName(spark, \"uc_data001.uc_schema001.fct_hr_employees\")\n",
    "\n",
    "fct_hr_employees_df.alias(\"t\") \\\n",
    "  .merge(\n",
    "    result_df.alias(\"s\"),\n",
    "    \"t.EMPLOYEE_ID = s.EMPLOYEE_ID \"\n",
    "  ) \\\n",
    "  .whenMatchedUpdate(set={\n",
    "    \"FIRST_NAME\": \"s.FIRST_NAME\",\n",
    "    \"LAST_NAME\": \"s.LAST_NAME\",\n",
    "    \"EMAIL\": \"s.EMAIL\",\n",
    "    \"PHONE_NUMBER\": \"s.PHONE_NUMBER\",\n",
    "    \"HIRE_DATE\": to_timestamp(col(\"s.HIRE_DATE\"), 'MM/dd/yyyy'),\n",
    "    \"JOB_ID\": \"s.JOB_ID\",\n",
    "    \"SALARY\": \"s.SALARY\",\n",
    "    \"MANAGER_ID\": \"s.MANAGER_ID\",\n",
    "    \"DEPARTMENT_ID\": \"s.DEPARTMENT_ID\",\n",
    "    \"DB_UPDATED_DATE\": current_timestamp()\n",
    "  }) \\\n",
    "  .whenNotMatchedInsert(values={\n",
    "    \"EMPLOYEE_ID\": \"s.EMPLOYEE_ID\",\n",
    "    \"FIRST_NAME\": \"s.FIRST_NAME\",\n",
    "    \"LAST_NAME\": \"s.LAST_NAME\",\n",
    "    \"EMAIL\": \"s.EMAIL\",\n",
    "    \"PHONE_NUMBER\": \"s.PHONE_NUMBER\",\n",
    "    \"HIRE_DATE\": to_timestamp(col(\"s.HIRE_DATE\"), 'MM/dd/yyyy'),\n",
    "    \"JOB_ID\": \"s.JOB_ID\",\n",
    "    \"SALARY\": \"s.SALARY\",\n",
    "    \"MANAGER_ID\": \"s.MANAGER_ID\",\n",
    "    \"DEPARTMENT_ID\": \"s.DEPARTMENT_ID\",\n",
    "    \"DB_CREATED_DATE\": current_timestamp(),\n",
    "    \"DB_UPDATED_DATE\": current_timestamp()\n",
    "  }) \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b80e3d-c241-4238-824b-a98b15e9e4da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = spark.table(\"uc_data001.uc_schema001.raw_hr_employees_countries\")  \n",
    "fct_hr_employees_df = DeltaTable.forName(spark, \"uc_data001.uc_schema001.dim_hr_employees_countries\")\n",
    "\n",
    "fct_hr_employees_df.alias(\"t\") \\\n",
    "  .merge(\n",
    "    result_df.alias(\"s\"),\n",
    "    \"t.COUNTRY_ID = s.COUNTRY_ID \"\n",
    "  ) \\\n",
    "  .whenMatchedUpdate(set={\n",
    "    \"COUNTRY_NAME\": \"s.COUNTRY_NAME\",\n",
    "    \"DB_UPDATED_DATE\": current_timestamp()\n",
    "  }) \\\n",
    "  .whenNotMatchedInsert(values={\n",
    "    \"COUNTRY_ID\": \"s.COUNTRY_ID\",\n",
    "    \"COUNTRY_NAME\": \"s.COUNTRY_NAME\",\n",
    "    \"DB_CREATED_DATE\": current_timestamp(),\n",
    "    \"DB_UPDATED_DATE\": current_timestamp()\n",
    "  }) \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fbc3304-349d-415e-8132-f6b4f261e1b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = spark.table(\"uc_data001.uc_schema001.raw_hr_employees_departments\")  \n",
    "fct_hr_employees_df = DeltaTable.forName(spark, \"uc_data001.uc_schema001.dim_hr_employees_departments\")\n",
    "\n",
    "fct_hr_employees_df.alias(\"t\") \\\n",
    "  .merge(\n",
    "    result_df.alias(\"s\"),\n",
    "    \"t.DEPARTMENT_ID = s.DEPARTMENT_ID \"\n",
    "  ) \\\n",
    "  .whenMatchedUpdate(set={\n",
    "    \"DEPARTMENT_NAME\": \"s.DEPARTMENT_NAME\",\n",
    "    \"MANAGER_ID\": \"s.MANAGER_ID\",\n",
    "    \"LOCATION_ID\": \"s.LOCATION_ID\",\n",
    "    \"DB_UPDATED_DATE\": current_timestamp()\n",
    "  }) \\\n",
    "  .whenNotMatchedInsert(values={\n",
    "    \"DEPARTMENT_ID\": \"s.DEPARTMENT_ID\",\n",
    "    \"DEPARTMENT_NAME\": \"s.DEPARTMENT_NAME\",\n",
    "    \"MANAGER_ID\": \"s.MANAGER_ID\",\n",
    "    \"LOCATION_ID\": \"s.LOCATION_ID\",\n",
    "    \"DB_CREATED_DATE\": current_timestamp(),\n",
    "    \"DB_UPDATED_DATE\": current_timestamp()\n",
    "  }) \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0d0ed5b-8f77-46ce-9c53-2cba3c8b002f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from uc_data001.uc_schema001.dim_hr_employees_departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6144e67-2999-402f-9e8f-4433dab82259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = spark.table(\"uc_data001.uc_schema001.raw_hr_employees_locations\")  # 员工表\n",
    "fct_hr_employees_df = DeltaTable.forName(spark, \"uc_data001.uc_schema001.dim_hr_employees_locations\")\n",
    "\n",
    "fct_hr_employees_df.alias(\"t\") \\\n",
    "  .merge(\n",
    "    result_df.alias(\"s\"),\n",
    "    \"t.LOCATION_ID = s.LOCATION_ID \"\n",
    "  ) \\\n",
    "  .whenMatchedUpdate(set={\n",
    "    \"STREET_ADDRESS\": \"s.STREET_ADDRESS\",\n",
    "    \"POSTAL_CODE\": \"s.POSTAL_CODE\",\n",
    "    \"CITY\": \"s.CITY\",\n",
    "    \"STATE_PROVINCE\": \"s.STATE_PROVINCE\",\n",
    "    \"COUNTRY_ID\": \"s.COUNTRY_ID\",\n",
    "    \"DB_UPDATED_DATE\": current_timestamp()\n",
    "  }) \\\n",
    "  .whenNotMatchedInsert(values={\n",
    "    \"LOCATION_ID\": \"s.LOCATION_ID\",\n",
    "    \"STREET_ADDRESS\": \"s.STREET_ADDRESS\",\n",
    "    \"POSTAL_CODE\": \"s.POSTAL_CODE\",\n",
    "    \"CITY\": \"s.CITY\",\n",
    "    \"STATE_PROVINCE\": \"s.STATE_PROVINCE\",\n",
    "    \"COUNTRY_ID\": \"s.COUNTRY_ID\",\n",
    "    \"DB_CREATED_DATE\": current_timestamp(),\n",
    "    \"DB_UPDATED_DATE\": current_timestamp()\n",
    "  }) \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57529ee4-954c-4eab-9192-88bd88d246da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %python\n",
    "# # Load the necessary tables into DataFrames\n",
    "# fct_hr_employees_df = spark.table(\"uc_data001.uc_schema001.fct_hr_employees\")\n",
    "# raw_hr_employees_df = spark.table(\"uc_data001.uc_schema001.raw_hr_employees\")\n",
    "# dim_hr_employees_countries_df = spark.table(\"uc_data001.uc_schema001.dim_hr_employees_countries\")\n",
    "# raw_hr_employees_countries_df = spark.table(\"uc_data001.uc_schema001.raw_hr_employees_countries\")\n",
    "# dim_hr_employees_departments_df = spark.table(\"uc_data001.uc_schema001.dim_hr_employees_departments\")\n",
    "# raw_hr_employees_departments_df = spark.table(\"uc_data001.uc_schema001.raw_hr_employees_departments\")\n",
    "# dim_hr_employees_locations_df = spark.table(\"uc_data001.uc_schema001.dim_hr_employees_locations\")\n",
    "# raw_hr_employees_locations_df = spark.table(\"uc_data001.uc_schema001.raw_hr_employees_locations\")\n",
    "\n",
    "# # Perform the merge operations using PySpark\n",
    "# from pyspark.sql.functions import current_timestamp, to_timestamp\n",
    "\n",
    "# # Ensure HIRE_DATE in fct_hr_employees_df is also a timestamp\n",
    "# fct_hr_employees_df = fct_hr_employees_df.withColumn(\n",
    "#     \"HIRE_DATE\", \n",
    "#     to_timestamp(fct_hr_employees_df.HIRE_DATE, 'MM/dd/yyyy')\n",
    "# )\n",
    "\n",
    "# # Merge for fct_hr_employees\n",
    "# merged_fct_hr_employees_df = raw_hr_employees_df.alias(\"t2\").join(\n",
    "#     fct_hr_employees_df.alias(\"t1\"),\n",
    "#     fct_hr_employees_df.EMPLOYEE_ID == raw_hr_employees_df.EMPLOYEE_ID,\n",
    "#     \"outer\"\n",
    "# ).select(\n",
    "#     raw_hr_employees_df.EMPLOYEE_ID,\n",
    "#     raw_hr_employees_df.FIRST_NAME,\n",
    "#     raw_hr_employees_df.LAST_NAME,\n",
    "#     raw_hr_employees_df.EMAIL,\n",
    "#     raw_hr_employees_df.PHONE_NUMBER,\n",
    "#     to_timestamp(raw_hr_employees_df.HIRE_DATE, 'MM/dd/yyyy').alias(\"HIRE_DATE\"),\n",
    "#     raw_hr_employees_df.JOB_ID,\n",
    "#     raw_hr_employees_df.SALARY,\n",
    "#     raw_hr_employees_df.MANAGER_ID,\n",
    "#     raw_hr_employees_df.DEPARTMENT_ID,\n",
    "#     current_timestamp().alias(\"DB_CREATED_DATE\"),\n",
    "#     current_timestamp().alias(\"DB_UPDATED_DATE\")\n",
    "# )\n",
    "\n",
    "# # Merge for dim_hr_employees_countries\n",
    "# merged_dim_hr_employees_countries_df = raw_hr_employees_countries_df.alias(\"t2\").join(\n",
    "#     dim_hr_employees_countries_df.alias(\"t1\"),\n",
    "#     dim_hr_employees_countries_df.COUNTRY_ID == raw_hr_employees_countries_df.COUNTRY_ID,\n",
    "#     \"outer\"\n",
    "# ).select(\n",
    "#     raw_hr_employees_countries_df.COUNTRY_ID,\n",
    "#     raw_hr_employees_countries_df.COUNTRY_NAME,\n",
    "#     current_timestamp().alias(\"DB_CREATED_DATE\"),\n",
    "#     current_timestamp().alias(\"DB_UPDATED_DATE\")\n",
    "# )\n",
    "\n",
    "# # Merge for dim_hr_employees_departments\n",
    "# merged_dim_hr_employees_departments_df = raw_hr_employees_departments_df.alias(\"t2\").join(\n",
    "#     dim_hr_employees_departments_df.alias(\"t1\"),\n",
    "#     dim_hr_employees_departments_df.DEPARTMENT_ID == raw_hr_employees_departments_df.DEPARTMENT_ID,\n",
    "#     \"outer\"\n",
    "# ).select(\n",
    "#     raw_hr_employees_departments_df.DEPARTMENT_ID,\n",
    "#     raw_hr_employees_departments_df.DEPARTMENT_NAME,\n",
    "#     raw_hr_employees_departments_df.MANAGER_ID,\n",
    "#     raw_hr_employees_departments_df.LOCATION_ID,\n",
    "#     current_timestamp().alias(\"DB_CREATED_DATE\"),\n",
    "#     current_timestamp().alias(\"DB_UPDATED_DATE\")\n",
    "# )\n",
    "\n",
    "# # Merge for dim_hr_employees_locations\n",
    "# merged_dim_hr_employees_locations_df = raw_hr_employees_locations_df.alias(\"t2\").join(\n",
    "#     dim_hr_employees_locations_df.alias(\"t1\"),\n",
    "#     dim_hr_employees_locations_df.LOCATION_ID == raw_hr_employees_locations_df.LOCATION_ID,\n",
    "#     \"outer\"\n",
    "# ).select(\n",
    "#     raw_hr_employees_locations_df.LOCATION_ID,\n",
    "#     raw_hr_employees_locations_df.STREET_ADDRESS,\n",
    "#     raw_hr_employees_locations_df.POSTAL_CODE,\n",
    "#     raw_hr_employees_locations_df.CITY,\n",
    "#     raw_hr_employees_locations_df.STATE_PROVINCE,\n",
    "#     raw_hr_employees_locations_df.COUNTRY_ID,\n",
    "#     current_timestamp().alias(\"DB_CREATED_DATE\"),\n",
    "#     current_timestamp().alias(\"DB_UPDATED_DATE\")\n",
    "# )\n",
    "\n",
    "# # Write the merged DataFrames back to the respective tables\n",
    "# merged_fct_hr_employees_df.write.mode(\"overwrite\").saveAsTable(\"uc_data001.uc_schema001.fct_hr_employees\")\n",
    "# merged_dim_hr_employees_countries_df.write.mode(\"overwrite\").saveAsTable(\"uc_data001.uc_schema001.dim_hr_employees_countries\")\n",
    "# merged_dim_hr_employees_departments_df.write.mode(\"overwrite\").saveAsTable(\"uc_data001.uc_schema001.dim_hr_employees_departments\")\n",
    "# merged_dim_hr_employees_locations_df.write.mode(\"overwrite\").saveAsTable(\"uc_data001.uc_schema001.dim_hr_employees_locations\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 103475353541116,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(pyspark) 02SILVER",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
